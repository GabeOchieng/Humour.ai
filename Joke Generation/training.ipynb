{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preliminaries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import AdamW, WarmUp, get_linear_schedule_with_warmup\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader,Dataset\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"device = 'cuda' # Selecting Device","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n  BATCH_SIZE = 16\n  EPOCHS = 4\n  LEARNING_RATE = 2e-5\n  MAX_LEN = 64\n  Tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Declaring the Model And adding Padding Token"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nspecial_tokens_dict = {'pad_token': '<PAD>'}\nnum_added_toks = config.Tokenizer.add_special_tokens(special_tokens_dict)\nprint('We have added', num_added_toks, 'tokens')\nmodel.resize_token_embeddings(len(config.Tokenizer)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Jokesdataset(Dataset):\n  def __init__(self,data,tokenizer):\n    self.data = data\n    self.tokenizer = tokenizer\n    self.eos_tok = \"<|endoftext|>\"\n    self.data['Joke'] = self.data['Joke'].apply(lambda x: \"JOKE:\" + str(x) + self.eos_tok) #Adding JOKE: at the start and EOS TOKEN at end\n\n  def __len__(self):\n    return len(self.data)\n\n  def __getitem__(self,idx):\n    joke = self.data.iloc[idx,1]\n    \n    inputs = self.tokenizer.encode_plus(\n            joke,\n            None,\n            add_special_tokens = True,\n            max_length = config.MAX_LEN,\n            pad_to_max_length = True\n        )\n\n    ids = inputs[\"input_ids\"]\n    mask = inputs[\"attention_mask\"]\n\n    return {'ids':torch.tensor(ids,dtype=torch.long),\n            'mask': torch.tensor(mask,dtype=torch.long),\n            'target':torch.tensor(ids,dtype=torch.long)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler,epoch):\n    model.train()\n    for bi, d in enumerate(data_loader):\n        ids = d[\"ids\"]\n        mask = d[\"mask\"]\n        labels = d['target']\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        labels = labels.to(device,dtype=torch.long)\n          \n        optimizer.zero_grad()\n        outputs = model(\n            input_ids =ids,\n            attention_mask=mask,\n            labels = labels\n        )\n\n        loss, logits = outputs[:2]                        \n        loss.backward()\n\n        optimizer.step()\n        if scheduler is not None:\n                scheduler.step()\n\n        if (bi+1) % 500 == 0:\n            print('Epoch [{}/{}], bi[{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, config.EPOCHS, bi+1,len(data_loader), loss.item()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Engine"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run():\n  jokes = pd.read_csv(\"/kaggle/input/short-jokes/shortjokes.csv\")\n\n  jokes_dataset = Jokesdataset(jokes,config.Tokenizer)\n  jokes_dataloader = DataLoader(jokes_dataset,\n                                batch_size=config.BATCH_SIZE,\n                                shuffle=True,\n                                num_workers=4)\n  \n  model.to(device)\n\n  num_train_steps = int(len(jokes_dataloader) / config.BATCH_SIZE / config.EPOCHS)\n\n  optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n  scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=num_train_steps)\n\n  for epoch in range(config.EPOCHS):\n        print(f\"EPOCH {epoch+1} started\" + '=' * 30)\n        train_fn(jokes_dataloader, model, optimizer, device, scheduler,epoch=epoch)\n        \n        models_folder = \"/kaggle/working/trained_models\"\n        if not os.path.exists(models_folder):\n          os.mkdir(models_folder)\n          torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_joker_{epoch}.pt\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"run()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}